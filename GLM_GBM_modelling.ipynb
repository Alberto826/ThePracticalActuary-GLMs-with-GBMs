{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Actuarial Neural Networks\n",
    "Author: Alberto Gutierrez\n",
    "\n",
    "This notebook demonstrates the implementation and evaluation of GLMS with Gradient Boosting Trees. The notebook also includes data preprocessing, model training, and performance evaluation using metrics such as Poisson Deviance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get Data\n",
    "The freMTPL2freq dataset will be used. The data set is read into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"./data/freMTPL2freq.csv\")\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyze the Data\n",
    "\n",
    "The sweetviz package is a great package to do a quick exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sweetviz as sv\n",
    "\n",
    "data_report = sv.analyze(data)\n",
    "data_report.show_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Prep\n",
    "\n",
    "This section defines the list of features, weight (exposure), and the target (response variable). It also splits the data into train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = ['Area','VehBrand','VehGas','Region','BonusMalus','VehPower','VehAge','DrivAge','Density']\n",
    "weight = 'Exposure'\n",
    "target = 'ClaimNb'\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=12345)\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "test_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the results \n",
    "print(\"Training DataFrame:\\n\", train_data.shape) \n",
    "print(\"Testing DataFrame:\\n\", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modelling\n",
    "\n",
    "We will be building various models including Null, GLM, GBM, GLM with GBM, Neural Net, CANN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Null Model\n",
    "\n",
    "The null model is simply the weighted average of the target, and the prediction is the weighted average multiplied by the weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_prediction = train_data[target].sum()/train_data[weight].sum()\n",
    "def null_predict(df):\n",
    "    return df[weight]*average_prediction\n",
    "def null_predict_std(df):\n",
    "    return null_predict(df)/df[weight]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['null_prediction'] = null_predict(test_data)\n",
    "train_data['null_prediction'] = null_predict(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. GLM\n",
    "\n",
    "There are many GLM packages such as glum, py-glm, pyglmnet, statsmodel, H2O, etc. Here we will be using the H2O python package. So lets initiate the H2O local server instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o\n",
    "h2o.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to do the following:\n",
    "1. Convert the Pandas dataframe to an H2O dataframe using\n",
    "2. Convert all numeric column sto categorical\n",
    "3. The log of exposure will be an input as an offset\n",
    "4. Set lambda_ to 0 to fit basic GLM instead of regularized GLM (ElasticNet)\n",
    "5. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.estimators import H2OGeneralizedLinearEstimator\n",
    "import numpy as np\n",
    "\n",
    "def h2o_preprocess(df):\n",
    "    h2o_df = h2o.H2OFrame(df)\n",
    "\n",
    "    def convert_to_categorical(cols):\n",
    "        for col in cols:\n",
    "            h2o_df[col]=h2o_df[col].asfactor()\n",
    "\n",
    "    convert_to_categorical(features)\n",
    "    h2o_df['offset'] = h2o_df[weight].log()\n",
    "    return h2o_df\n",
    "\n",
    "h2o_train = h2o_preprocess(train_data)\n",
    "h2o_test = h2o_preprocess(test_data)\n",
    "\n",
    "glm_model = H2OGeneralizedLinearEstimator(\n",
    "    family=\"poisson\", link=\"Log\",\n",
    "    lambda_= 0, \n",
    "    seed=123\n",
    "\n",
    ")\n",
    "\n",
    "glm_model.train(\n",
    "    offset_column='offset',\n",
    "    training_frame = h2o_train,\n",
    "    validation_frame= h2o_test,\n",
    "    x=features,\n",
    "    y=target\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now get predicted values from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glm_predict(df):\n",
    "    preprocessed_df = h2o_preprocess(df)\n",
    "    predictions = glm_model.predict(preprocessed_df).as_data_frame()\n",
    "    return predictions['predict'].values\n",
    "def glm_predict_std(df):\n",
    "    predictions = glm_predict(df)/df[weight]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['glm_prediction'] = glm_predict(test_data)\n",
    "train_data['glm_prediction'] = glm_predict(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "def xgb_preprocess(df):\n",
    "    xgb_df = df.copy()\n",
    "    # Convert object types to categorical\n",
    "    for col in xgb_df.select_dtypes(include='object').columns:\n",
    "        xgb_df[col] = xgb_df[col].astype('category')\n",
    "    base_margin = np.log(xgb_df[weight])\n",
    "    xgb_dfM = xgb.DMatrix(xgb_df[features], label=xgb_df[target], enable_categorical=True, base_margin=base_margin)\n",
    "    return xgb_dfM\n",
    "\n",
    "xgb_train = xgb_preprocess(train_data)\n",
    "xgb_test = xgb_preprocess(test_data)\n",
    "\n",
    "params = {\n",
    "    'objective': 'count:poisson',  # For count data with Poisson distribution\n",
    "    'booster': 'gbtree',\n",
    "    'max_depth': 6,\n",
    "    'eta': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'nthread': 4,\n",
    "    'learning_rate': 0.1,\n",
    "    'seed': 123,\n",
    "}\n",
    "\n",
    "# Train the XGBoost model\n",
    "watchlist = [(xgb_train, 'train'), (xgb_test, 'val')]\n",
    "num_rounds = 200\n",
    "xgb_model = xgb.train(params, xgb_train, num_rounds, evals=watchlist, early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now get the predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_predict(df):\n",
    "    xgb_df = xgb_preprocess(df)\n",
    "    return xgb_model.predict(xgb_df)\n",
    "def xgb_predict_std(df):\n",
    "    return xgb_predict(df)/df[weight]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['xgb_prediction'] = xgb_predict(test_data)\n",
    "train_data['xgb_prediction'] = xgb_predict(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. GLM-GBM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "def glm_xgb_preprocess(df):\n",
    "    xgb_df = df.copy()\n",
    "    # Convert object types to categorical\n",
    "    for col in xgb_df.select_dtypes(include='object').columns:\n",
    "        xgb_df[col] = xgb_df[col].astype('category')\n",
    "    base_margin = np.log(xgb_df['glm_prediction'])\n",
    "    xgb_dfM = xgb.DMatrix(xgb_df[features], label=xgb_df[target], enable_categorical=True, base_margin=base_margin)\n",
    "    return xgb_dfM\n",
    "\n",
    "xgb_train = glm_xgb_preprocess(train_data)\n",
    "xgb_test = glm_xgb_preprocess(test_data)\n",
    "\n",
    "params = {\n",
    "    'objective': 'count:poisson',  # For count data with Poisson distribution\n",
    "    'booster': 'gbtree',\n",
    "    'max_depth': 6,\n",
    "    'eta': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'nthread': 4,\n",
    "    'learning_rate': 0.1,\n",
    "    'seed': 123,\n",
    "}\n",
    "\n",
    "# Train the XGBoost model\n",
    "watchlist = [(xgb_train, 'train'), (xgb_test, 'val')]\n",
    "num_rounds = 200\n",
    "glm_xgb_model = xgb.train(params, xgb_train, num_rounds, evals=watchlist, early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glm_xgb_predict(df):\n",
    "    xgb_df = glm_xgb_preprocess(df)\n",
    "    predictions = glm_xgb_model.predict(xgb_df)\n",
    "    return predictions\n",
    "def glm_xgb_predict_std(df):\n",
    "    predictions = glm_xgb_predict(df)/df[weight]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['glm_xgb_prediction'] = glm_xgb_predict(test_data)\n",
    "train_data['glm_xgb_prediction'] = glm_xgb_predict(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Neural Network\n",
    "\n",
    "We will be using pytorch (using the lightning API) to build and train our neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by defining our data class which does the following:\n",
    "1. Initialise the instance based on the inputs: pandas dataframe (train and test), weight, target variables, the prior prediction value (for impleenting CANNs) and batch size.\n",
    "2. Preprocess the data by standardizing the numeric variables and one-hot encoding the categorical variables and converting to a pytorch tensor dataset\n",
    "3. Create dataloaders for the train and test set which will return batches from teh tensor during traiing and validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':16:8'\n",
    "\n",
    "import pytorch_lightning as L\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "\n",
    "class CustomDataModule(L.LightningDataModule):\n",
    "    def __init__(self, train_data, test_data, features, target, weight, prior_prediction=None, batch_size=10000):\n",
    "        super().__init__()\n",
    "        self.nn_train = train_data.copy()\n",
    "        self.nn_test = test_data.copy()\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.weight = weight\n",
    "        self.batch_size = batch_size\n",
    "        self.prior_prediction = prior_prediction\n",
    "        self.fit_transform_pipeline(self.nn_train)\n",
    "        self.preprocess_data()\n",
    "\n",
    "    def fit_transform_pipeline(self, data):\n",
    "        # Identify categorical and numerical columns\n",
    "        cat_cols = [col for col in data.select_dtypes(include=['object']).columns \n",
    "                   if col in self.features]\n",
    "        num_cols = [col for col in data.select_dtypes(include=['int64', 'float64']).columns \n",
    "                   if col in self.features]\n",
    "\n",
    "        # Create ColumnTransformer\n",
    "        pipeline = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', StandardScaler(), num_cols),\n",
    "                ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)\n",
    "            ]\n",
    "        )\n",
    "        self.pipeline = pipeline.fit(data)\n",
    "        self.pipeline_features = num_cols + list(pipeline.named_transformers_['cat'].get_feature_names_out(cat_cols))\n",
    "\n",
    "    def preprocess(self, data):\n",
    "        transformed_data = pd.DataFrame(self.pipeline.transform(data), columns=self.pipeline_features)\n",
    "        X = torch.tensor(transformed_data.values, dtype=torch.float32)\n",
    "        y = torch.tensor(data[self.target].values, dtype=torch.float32)\n",
    "        w = torch.tensor(data[self.weight].values, dtype=torch.float32)\n",
    "        if self.prior_prediction:\n",
    "            p = torch.tensor(data[self.prior_prediction].values, dtype=torch.float32)\n",
    "        else:\n",
    "            p = w\n",
    "        dataset = TensorDataset(X, y, w, p)\n",
    "        return X, y, w, p, dataset\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        self.X_train, self.y_train, self.w_train, self.p_train, self.train_dataset = self.preprocess(self.nn_train)\n",
    "        self.X_test, self.y_test, self.w_test, self.p_test, self.test_dataset = self.preprocess(self.nn_test)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=12, persistent_workers=True, pin_memory=True, drop_last=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=12, persistent_workers=True, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the neural net model with an exponential activation as the final output that uses the Poisson Deviance loss function.\n",
    "When this output is multiplied by the prior prediction (such as the GLM perediction) before being calculated in teh loss function, then the model becomes the CANN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "class CANN(L.LightningModule):\n",
    "\n",
    "    class PoissonDevianceLoss(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "        \n",
    "        def forward(self, y_pred, y_true):\n",
    "            y_true_e = y_true + 1e-16\n",
    "            return torch.mean(2 * (y_true * torch.log(y_true_e / y_pred) - (y_true - y_pred)))\n",
    "\n",
    "    class ExpActivation(nn.Module): \n",
    "        def forward(self, x): \n",
    "            return torch.exp(x)\n",
    "        \n",
    "    class ResBlock(nn.Module):\n",
    "        def __init__(self, input_dim):\n",
    "            super().__init__()\n",
    "            self.fc1 = nn.Linear(input_dim, input_dim)\n",
    "            self.fc2 = nn.Linear(input_dim, input_dim)\n",
    "            self.dropout = nn.Dropout(p=0.2)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            residual = x\n",
    "            out = F.relu(self.fc1(x))\n",
    "            out = self.dropout(out)\n",
    "            out = self.fc2(out)\n",
    "            out = F.relu(out + residual)\n",
    "            return out\n",
    "        \n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.NeuralNet = nn.Sequential(\n",
    "            *[self.ResBlock(input_dim) for _ in range(10)],\n",
    "            nn.Linear(input_dim, 1), self.ExpActivation(),\n",
    "        )\n",
    "        # Manually set the weights of the final Linear layer to zero\n",
    "        self.NeuralNet[-2].weight.data.fill_(0)\n",
    "        self.NeuralNet[-2].bias.data.fill_(0)\n",
    "        \n",
    "        self.loss_fn = self.PoissonDevianceLoss()\n",
    "        \n",
    "    def forward(self, x, p=1):\n",
    "        cann = self.NeuralNet(x).view(-1) * p\n",
    "        return cann\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, w, p = batch\n",
    "        y_pred = self.forward(x, p)\n",
    "        loss = self.loss_fn(y_pred, y)\n",
    "        self.log('train_loss', loss, on_step=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, w, p = batch\n",
    "        y_pred = self.forward(x, p)\n",
    "        loss = self.loss_fn(y_pred, y)\n",
    "        self.log('val_loss', loss, on_step=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.NAdam(self.parameters())\n",
    "\n",
    "# Training setup and execution\n",
    "def train_model(train_data, test_data, features, target, weight, prior_prediction=None, max_epochs=100, batch_size=10000):\n",
    "    L.seed_everything(123, workers=True)\n",
    "    # Initialize data module\n",
    "    data_module = CustomDataModule(train_data, test_data, features, target, weight, \n",
    "                                   prior_prediction=prior_prediction, batch_size=batch_size)\n",
    "    # Initialize model\n",
    "    input_dim=data_module.X_train.size(dim=1)\n",
    "    model = CANN(input_dim)\n",
    "    \n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss_epoch',\n",
    "        patience=5,\n",
    "        min_delta=1e-5,\n",
    "        mode='min'\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        accelerator='auto', \n",
    "        callbacks=[early_stopping],\n",
    "        enable_progress_bar=True,\n",
    "        deterministic=True\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.fit(model, data_module)    \n",
    "    return model, trainer, data_module\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the NN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model, trainer, data_module = train_model(\n",
    "    train_data=train_data,\n",
    "    test_data=test_data,\n",
    "    features=features,\n",
    "    target=target,\n",
    "    weight=weight\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now get the predicted values from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_predict(df):\n",
    "    X, y, w, p, dataset = data_module.preprocess(df)\n",
    "    nn_model.eval()\n",
    "    predictions = nn_model(X).detach().numpy()*df[weight]\n",
    "    return predictions\n",
    "def nn_predict_std(df):\n",
    "    predictions = nn_predict(df)/df[weight]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['nn_prediction']=nn_predict(test_data)\n",
    "train_data['nn_prediction']=nn_predict(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. CANN\n",
    "Using the same function and model class to train the model, set the prior_prediction parameter to the column name that contains the prior prediction, e.g. GLM prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cann_model, cann_trainer, cann_data_module = train_model(\n",
    "    train_data=train_data,\n",
    "    test_data=test_data,\n",
    "    features=features,\n",
    "    target=target,\n",
    "    weight=weight,\n",
    "    prior_prediction='glm_prediction',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get predicted values from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cann_predict(df):\n",
    "    X, y, w, p, dataset = cann_data_module.preprocess(df)\n",
    "    cann_model.eval()\n",
    "    predictions = cann_model(X).detach().numpy()*df['glm_prediction']\n",
    "    return predictions\n",
    "def cann_predict_std(df):\n",
    "    predictions = cann_predict(df)/df[weight]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['cann_prediction']=cann_predict(test_data)\n",
    "train_data['cann_prediction']=cann_predict(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Deviance & Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_diagnostics.scoring import decompose, PoissonDeviance\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_eval = test_data.copy()\n",
    "models = ['null_prediction','glm_prediction','xgb_prediction','glm_xgb_prediction','nn_prediction','cann_prediction']\n",
    "model_prediction_functions = {\n",
    "    'null_prediction': null_predict_std, \n",
    "    'glm_prediction': glm_predict_std,\n",
    "    'xgb_prediction': xgb_predict_std,\n",
    "    'glm_xgb_prediction': glm_xgb_predict_std,\n",
    "    'nn_prediction': nn_predict_std,\n",
    "    'cann_prediction': cann_predict_std,\n",
    "}\n",
    "\n",
    "PoissonDeviance_scoring = PoissonDeviance()\n",
    "\n",
    "def gini_coefficient(y_true, y_pred):\n",
    "    sorted_indices = np.argsort(y_pred)\n",
    "    y_true_sorted = y_true[sorted_indices]\n",
    "    lorenz_curve = np.cumsum(y_true_sorted) / np.sum(y_true_sorted)\n",
    "    lorenz_curve = np.insert(lorenz_curve, 0, 0)\n",
    "    gini = 1 - 2 * np.trapz(lorenz_curve, dx=1/len(y_true))\n",
    "    return gini\n",
    "\n",
    "\n",
    "model_eval_dict={}\n",
    "\n",
    "for model in models:\n",
    "    PoissonDeviance_score = PoissonDeviance_scoring(y_obs=data_eval[target],y_pred=data_eval[model])\n",
    "    true_gini = gini_coefficient((data_eval[target]/data_eval[weight]).values, (data_eval[target]/data_eval[weight]).values)\n",
    "    gini = gini_coefficient((data_eval[target]/data_eval[weight]).values, (data_eval[model]/data_eval[weight]).values)\n",
    "    model_eval_dict[model]={\n",
    "        'PoissonDeviance': PoissonDeviance_score, \n",
    "        'ActualAverageResponse': data_eval[target].sum()/data_eval[weight].sum(),\n",
    "        'PredictedAverageResponse': data_eval[model].sum()/data_eval[weight].sum(),\n",
    "        'ActualTotalResponse': data_eval[target].sum(),\n",
    "        'PredictedTotalResponse': data_eval[model].sum(),\n",
    "        'TrueGiniCoefficient': true_gini,\n",
    "        'GiniCoefficient': gini,\n",
    "        }\n",
    "    \n",
    "pd.DataFrame(model_eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Reliability Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_diagnostics import config_context\n",
    "from model_diagnostics.calibration import plot_marginal, plot_reliability_diagram\n",
    "\n",
    "\n",
    "with config_context(plot_backend=\"plotly\"):\n",
    "    fig = plot_reliability_diagram(\n",
    "        y_obs=data_eval[target].div(data_eval[weight], axis=0),\n",
    "        y_pred=data_eval[models].div(data_eval[weight], axis=0),\n",
    "        weights=data_eval[weight],\n",
    "    )\n",
    "fig.show(renderer=\"notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Model Bias Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_diagnostics import config_context\n",
    "from model_diagnostics.calibration import plot_bias\n",
    "\n",
    "\n",
    "with config_context(plot_backend=\"plotly\"):\n",
    "    fig = plot_bias(\n",
    "        y_obs=data_eval[target]/data_eval[weight],\n",
    "        y_pred=data_eval[models].div(data_eval[weight], axis=0),\n",
    "        weights=data_eval[weight],\n",
    "        n_bins=30,\n",
    "    )\n",
    "fig.show(renderer=\"notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Bias Plots by Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_diagnostics import config_context\n",
    "from model_diagnostics.calibration import plot_bias\n",
    "\n",
    "for col in features:\n",
    "    with config_context(plot_backend=\"plotly\"):\n",
    "        fig = plot_bias(\n",
    "            y_obs=data_eval[target]/data_eval[weight],\n",
    "            y_pred=data_eval[models].div(data_eval[weight], axis=0),\n",
    "            weights=data_eval[weight],\n",
    "            feature=data_eval[col],\n",
    "            n_bins=30,\n",
    "        )\n",
    "    fig.show(renderer=\"notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5. Marginal/Partial Dependency Plots by Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_diagnostics import config_context\n",
    "from model_diagnostics.calibration import compute_marginal\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "h2o.no_progress()\n",
    "\n",
    "for feature in features:\n",
    "    fig = go.Figure(\n",
    "        layout=dict( \n",
    "            title=f'Marginal Plots for feature {feature}', \n",
    "            xaxis_title=feature, \n",
    "            yaxis_title='Sum of Weights', \n",
    "            yaxis2=dict(title='Average Value', overlaying='y', side='right'), \n",
    "            barmode='group' \n",
    "        )\n",
    "    )\n",
    "    for i, model in enumerate(models):\n",
    "        marginals = compute_marginal(\n",
    "            y_obs=data_eval[target]/data_eval[weight],\n",
    "            y_pred=data_eval[model]/data_eval[weight],\n",
    "            X=data_eval,\n",
    "            weights=data_eval[weight],\n",
    "            feature_name=feature,\n",
    "            # predict_function=model_prediction_functions[model], #this is for PDP plots\n",
    "            # n_max=10000, #this is for PDP plots\n",
    "            n_bins=30,\n",
    "        )\n",
    "        if i==0:\n",
    "            fig.add_trace(go.Bar(x=marginals[feature], y=marginals[\"weights\"],name='Weights'))\n",
    "            fig.add_trace( go.Scatter(x=marginals[feature],y=marginals[\"y_obs_mean\"],mode='lines+markers',name='y_obs_mean',yaxis='y2',line=dict(dash='dot')))\n",
    "        fig.add_trace(go.Scatter(x=marginals[feature],y=marginals[\"y_pred_mean\"],mode='lines+markers',name=model,yaxis='y2'))\n",
    "        # fig.add_trace(go.Scatter(x=marginals[feature],y=marginals[\"partial_dependence\"],mode='lines+markers',name=model,yaxis='y2')) #this is for PDP plots\n",
    "    fig.show(renderer=\"notebook\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Analyse GBM boosting component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1. Distribution of boosting component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Plot density graph using plotly\n",
    "fig = ff.create_distplot([data_eval['glm_xgb_prediction'] / data_eval['glm_prediction']], group_labels=['XGB Boosting Component'])\n",
    "fig.update_layout(\n",
    "    title='Density Plot of XGB Boosting Component',\n",
    "    xaxis_title='XGB Boosting Component',\n",
    "    yaxis_title='Density',\n",
    "    xaxis=dict(range=[0, 5])\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2. Data Profile Comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "boosted_model = 'glm_xgb_prediction'\n",
    "# Function to apply filters\n",
    "def create_bins(df, feature):\n",
    "    # Compute histogram'\n",
    "    hist_data_orig, bin_edges_orig = np.histogram(df[feature], bins='auto')\n",
    "    # Build x values\n",
    "    x_values = np.array([(bin_edges_orig[i] + bin_edges_orig[i + 1]) / 2 for i in range(len(bin_edges_orig) - 1)])\n",
    "    # Create a DataFrame with bin edges and x values\n",
    "    bins_df = pd.DataFrame({'bin_start': bin_edges_orig[:-1], 'bin_end': bin_edges_orig[1:], 'x': x_values})\n",
    "    bins_df['bin_start'] = bins_df['bin_start'].astype(float)\n",
    "    return bins_df\n",
    "\n",
    "def plot_graphs(fig, df, feature, bins, lower_threshold, upper_threshold, color):\n",
    "    filtered_df = df[(df[boosted_model] / df['glm_prediction'] > lower_threshold) & (df[boosted_model] / df['glm_prediction'] <= upper_threshold)]\n",
    "    if pd.api.types.is_numeric_dtype(df[feature]):\n",
    "        filtered_df[feature] = filtered_df[feature].astype(float)\n",
    "        # Merge with the original DataFrame\n",
    "        merged_df = pd.merge_asof(filtered_df.sort_values(by=feature), bins[['bin_start', 'x']], left_on=feature, right_on='bin_start')\n",
    "        # Group by x values and add traces\n",
    "        grouped = merged_df.groupby('x')\n",
    "        # Add traces\n",
    "        fig.add_traces([\n",
    "            go.Bar(x=grouped.size().index,y=grouped[weight].sum()/filtered_df[weight].sum(),name=f'Weights {lower_threshold} < x <= {upper_threshold}',opacity=0.3, marker=dict(color=color)),\n",
    "            go.Scatter(x=grouped.size().index,y=grouped[target].sum()/grouped['glm_prediction'].sum(),\n",
    "                mode='lines+markers',line=dict(dash='dot'),marker=dict(color=color),\n",
    "                name=f'GLM AvE ({lower_threshold} < x <= {upper_threshold})', \n",
    "                yaxis='y2'\n",
    "            ),\n",
    "            go.Scatter(x=grouped.size().index,y=grouped[target].sum() / grouped[boosted_model].sum(),\n",
    "                mode='lines+markers', marker=dict(color=color),\n",
    "                name=f'GLM with Boosting AvE ({lower_threshold} < x <= {upper_threshold})', \n",
    "                yaxis='y2'\n",
    "            )\n",
    "        ])\n",
    "    else:\n",
    "        grouped_filtered = filtered_df.groupby(feature)[[target,'glm_prediction',boosted_model,weight]].sum().reset_index()\n",
    "        fig.add_trace(go.Bar(x=grouped_filtered[feature],y=grouped_filtered[weight]/grouped_filtered[weight].sum(),marker=dict(color=color), name=f'Weights ({lower_threshold} < x <= {upper_threshold})', opacity=0.3))\n",
    "        fig.add_trace(go.Scatter(x=grouped_filtered[feature],y=grouped_filtered[target]/grouped_filtered['glm_prediction'],mode='lines+markers',marker=dict(color=color),line=dict(dash='dot'),name=f'GLM AvE ({lower_threshold} < x <= {upper_threshold})',yaxis='y2'))\n",
    "        fig.add_trace(go.Scatter(x=grouped_filtered[feature],y=grouped_filtered[target]/grouped_filtered[boosted_model],mode='lines+markers',marker=dict(color=color),name=f'GLM with Boosting AvE ({lower_threshold} < x <= {upper_threshold})',yaxis='y2'))\n",
    "    return fig\n",
    "# Function to create combined bar chart\n",
    "def create_combined_bar_chart(feature, lower_1, upper_1, lower_2, upper_2):\n",
    "    clear_output(wait=True)\n",
    "    display(feature_selector, lower_slider_1, upper_slider_1, lower_slider_2, upper_slider_2, update_button)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    bins = None\n",
    "    if pd.api.types.is_numeric_dtype(data_eval[feature]):\n",
    "        bins = create_bins(data_eval, feature)\n",
    "    fig = plot_graphs(fig, data_eval, feature, bins, lower_1, upper_1, 'green')\n",
    "    fig = plot_graphs(fig, data_eval, feature, bins, lower_2, upper_2, 'red')\n",
    "\n",
    "    # Update layout for better visualization\n",
    "    fig.update_layout(\n",
    "        title=f'Distribution of {feature} (Subset 1 vs Subset 2) where subset is based on x=Booster Component',\n",
    "        barmode='overlay',\n",
    "        xaxis_title=feature,\n",
    "        yaxis_title='Percent',\n",
    "        yaxis2=dict(title='AvE Values', overlaying='y', side='right'), \n",
    "        legend_title='Dataset',\n",
    "        bargap=0.2,\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "# Dropdown widget for feature selection\n",
    "feature_selector = widgets.Dropdown(options=features,description='Feature:',disabled=False)\n",
    "\n",
    "# Slider widgets for original dataset threshold selection\n",
    "lower_slider_1 = widgets.FloatSlider(value=0, min=0, max=2, step=0.1,description='1 Lower Threshold:',continuous_update=False)\n",
    "upper_slider_1 = widgets.FloatSlider(value=0.95, min=0, max=2, step=0.1,description='1 Upper Threshold:',continuous_update=False)\n",
    "\n",
    "# Slider widgets for filtered dataset threshold selection\n",
    "lower_slider_2 = widgets.FloatSlider(value=1.05, min=0, max=2, step=0.1,description='2 Lower Threshold:',continuous_update=False)\n",
    "upper_slider_2 = widgets.FloatSlider(value=2, min=0, max=2, step=0.1,description='2 Upper Threshold:',continuous_update=False)\n",
    "\n",
    "# Button to update plot\n",
    "update_button = widgets.Button(description=\"Update Plot\")\n",
    "\n",
    "def on_button_click(b):\n",
    "    create_combined_bar_chart(feature_selector.value, lower_slider_1.value, upper_slider_1.value, lower_slider_2.value, upper_slider_2.value)\n",
    "\n",
    "update_button.on_click(on_button_click)\n",
    "\n",
    "# Display widgets\n",
    "display(feature_selector, lower_slider_1, upper_slider_1, lower_slider_2, upper_slider_2, update_button)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_diagnostics import config_context\n",
    "from model_diagnostics.calibration import compute_marginal\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "for feature in features:\n",
    "    fig = go.Figure(\n",
    "        layout=dict( \n",
    "            title=f'Marginal Plots for feature {feature}', \n",
    "            xaxis_title=feature, \n",
    "            yaxis_title='Sum of Weights', \n",
    "            yaxis2=dict(title='Average Value', overlaying='y', side='right'), \n",
    "            barmode='group' \n",
    "        )\n",
    "    )\n",
    "    marginals = compute_marginal(\n",
    "        y_obs=data_eval[boosted_model]/data_eval[boosted_model],\n",
    "        y_pred=data_eval[boosted_model]/data_eval['glm_prediction'],\n",
    "        X=data_eval,\n",
    "        weights=data_eval[weight],\n",
    "        feature_name=feature,\n",
    "        # predict_function=model_prediction_functions[boosted_model],\n",
    "        # n_max=100000,\n",
    "        n_bins=30,\n",
    "    )\n",
    "    fig.add_trace(go.Bar(x=marginals[feature], y=marginals[\"weights\"], name='Weights'))\n",
    "    fig.add_trace(go.Scatter(x=marginals[feature], y=marginals[\"y_obs_mean\"], mode='lines+markers', name='y_obs_mean', yaxis='y2', line=dict(dash='dot')))\n",
    "    fig.add_trace(go.Scatter(x=marginals[feature], y=marginals[\"y_pred_mean\"],error_y=dict(type='data', array=marginals[\"y_pred_stderr\"]*2), mode='lines+markers', name=boosted_model, yaxis='y2'))\n",
    "    # fig.add_trace(go.Scatter(x=marginals[feature], y=marginals[\"partial_dependence\"], mode='lines+markers', name=\"Partial Dependence\", yaxis='y2'))\n",
    "    fig.show(renderer=\"notebook\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_cann",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
